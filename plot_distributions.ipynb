{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML4HPC: Ensemble of Forecasters\n",
    "\n",
    "### Team Members:\n",
    "- Luca Venerando Greco\n",
    "- Bice Marzagora\n",
    "- Elia Vaglietti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "In this notebook, we start by importing several essential libraries that are used throughout the workflow:\n",
    "\n",
    "- `os`: Provides functions for interacting with the operating system, such as creating directories and handling file paths.\n",
    "- `sys`: Provides access to some variables used or maintained by the Python interpreter and to functions that interact strongly with the interpreter.\n",
    "- `matplotlib.pyplot`: A plotting library used for creating static, animated, and interactive visualizations in Python.\n",
    "- `numpy`: A fundamental package for scientific computing with Python, used for working with arrays and matrices.\n",
    "- `time`: Provides various time-related functions.\n",
    "- `tqdm`: A library for creating progress bars and progress meters.\n",
    "\n",
    "These libraries are crucial for tasks such as data manipulation, visualization, and managing the execution of jobs in a high-performance computing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Setup and Job Configuration\n",
    "\n",
    "We now set up the necessary directories and define the job configurations. Specifically, we create folders for storing charts, data, and logs if they do not already exist. We also define the number of forecasters and nodes for different scaling tests.\n",
    "\n",
    "If no new data is needed, set the `GENERATE_DATA` variable to `False` to skip the data generation step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "charts_folder = \"charts\"\n",
    "data_folder = \"data\"\n",
    "logs_folder = \"logs\"\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "if not os.path.exists(logs_folder):\n",
    "    os.makedirs(logs_folder)\n",
    "\n",
    "if not os.path.exists(charts_folder):\n",
    "    os.makedirs(charts_folder)\n",
    "\n",
    "one_milion_forecasters = int(1e5)\n",
    "strong_scaling_forecasters = int(1e4)\n",
    "weak_scaling_forecasters = int(1e4)\n",
    "\n",
    "one_milion_nodes = 10\n",
    "strong_scaling_nodes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Number of nodes to test\n",
    "weak_scaling_nodes   = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Number of nodes to test\n",
    "\n",
    "n_runs = 5\n",
    "\n",
    "GENERATE_DATA = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Submission Function\n",
    "\n",
    "We define a function `submit_job` that handles the submission of jobs to the scheduler. This function takes the number of nodes, the number of forecasters, and a job name as input parameters. It creates the necessary directories for storing data and logs, reads a template launch script, formats it with the provided parameters, writes the formatted script to a file, and submits the job using the `sbatch` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_job(num_nodes, num_forecasters, job_name):\n",
    "    num_tasks_per_node = 128\n",
    "\n",
    "    if num_tasks_per_node > 128:\n",
    "        print(\"The number of tasks per node should be less than or equal to 128\")\n",
    "        exit(1)\n",
    "\n",
    "    if not os.path.exists(f\"{data_folder}/{job_name}\"):\n",
    "        os.makedirs(f\"{data_folder}/{job_name}\")\n",
    "\n",
    "    if not os.path.exists(f\"{logs_folder}/{job_name}\"):\n",
    "        os.makedirs(f\"{logs_folder}/{job_name}\")\n",
    "\n",
    "    with open('launch.sh', 'r') as file:\n",
    "        launch_script = file.read()\n",
    "\n",
    "    launch_script = launch_script.format(\n",
    "        num_nodes=num_nodes,\n",
    "        num_tasks_per_node=num_tasks_per_node,\n",
    "        current_dir=current_dir,\n",
    "        world_size=num_nodes*num_tasks_per_node,\n",
    "        num_forecasters=num_forecasters,\n",
    "        data_folder=f\"{data_folder}/{job_name}\",\n",
    "        logs_folder=f\"{logs_folder}/{job_name}\"\n",
    "    )\n",
    "\n",
    "    script_filename = f\"{logs_folder}/{job_name}/launch.sh\"\n",
    "    with open(script_filename, \"w\") as script_file:\n",
    "        script_file.write(launch_script)\n",
    "\n",
    "    os.system(f\"sbatch {script_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Test Functions\n",
    "\n",
    "In the following sections, we define functions to run different scalability tests. These functions will help us automate the process of submitting jobs for one million forecasters, strong scaling, and weak scaling tests. Each function will generate a unique job name, submit the job using the `submit_job` function, and return the job names for tracking purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_milion_test():\n",
    "    job_names = []\n",
    "    for i in range(n_runs):\n",
    "        run_dir = f\"{data_folder}/one_milion_test/run_{i}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.makedirs(run_dir)\n",
    "        \n",
    "        job_name = f\"/one_milion_test/run_{i}\"\n",
    "\n",
    "        submit_job(one_milion_nodes, one_milion_forecasters, job_name)\n",
    "\n",
    "        job_names.append(job_name)\n",
    "\n",
    "    return job_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strong_scaling():\n",
    "    job_names = []\n",
    "    for run in range(1):\n",
    "        for num_nodes in strong_scaling_nodes:\n",
    "            job_name = f\"/strong_scaling/run_{run}/nodes_{num_nodes}\"\n",
    "            submit_job(num_nodes, strong_scaling_forecasters, job_name)\n",
    "            job_names.append(job_name)\n",
    "    return job_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_scaling():\n",
    "    job_names = []\n",
    "    for run in range(1):\n",
    "        for num_nodes in weak_scaling_nodes:\n",
    "            job_name = f\"/weak_scaling/run_{run}/nodes_{num_nodes}_forecasters_{weak_scaling_forecasters*num_nodes}\"\n",
    "            submit_job(num_nodes, weak_scaling_forecasters*num_nodes, job_name)\n",
    "            job_names.append(job_name)\n",
    "    return job_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waiting for jobs\n",
    "\n",
    "Now we wait for all the jobs to complete, in the meantime the `tqdm` progress bar will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 4465155\n",
      "Submitted batch job 4465156\n",
      "Submitted batch job 4465157\n",
      "Submitted batch job 4465158\n",
      "Submitted batch job 4465159\n",
      "Submitted batch job 4465160\n",
      "Submitted batch job 4465161\n",
      "Submitted batch job 4465162\n",
      "Submitted batch job 4465163\n",
      "Submitted batch job 4465164\n",
      "Submitted batch job 4465165\n",
      "Submitted batch job 4465166\n",
      "Submitted batch job 4465167\n",
      "Submitted batch job 4465168\n",
      "Submitted batch job 4465169\n",
      "Submitted batch job 4465170\n",
      "Submitted batch job 4465171\n",
      "Submitted batch job 4465172\n",
      "Submitted batch job 4465173\n",
      "Submitted batch job 4465174\n",
      "Submitted batch job 4465175\n",
      "Submitted batch job 4465176\n",
      "Submitted batch job 4465177\n",
      "Submitted batch job 4465178\n",
      "Submitted batch job 4465179\n",
      "Waiting for jobs to finish...\n",
      "['/one_milion_test/run_0', '/one_milion_test/run_1', '/one_milion_test/run_2', '/one_milion_test/run_3', '/one_milion_test/run_4', '/strong_scaling/run_0/nodes_1', '/strong_scaling/run_0/nodes_2', '/strong_scaling/run_0/nodes_3', '/strong_scaling/run_0/nodes_4', '/strong_scaling/run_0/nodes_5', '/strong_scaling/run_0/nodes_6', '/strong_scaling/run_0/nodes_7', '/strong_scaling/run_0/nodes_8', '/strong_scaling/run_0/nodes_9', '/strong_scaling/run_0/nodes_10', '/weak_scaling/run_0/nodes_1_forecasters_10000', '/weak_scaling/run_0/nodes_2_forecasters_20000', '/weak_scaling/run_0/nodes_3_forecasters_30000', '/weak_scaling/run_0/nodes_4_forecasters_40000', '/weak_scaling/run_0/nodes_5_forecasters_50000', '/weak_scaling/run_0/nodes_6_forecasters_60000', '/weak_scaling/run_0/nodes_7_forecasters_70000', '/weak_scaling/run_0/nodes_8_forecasters_80000', '/weak_scaling/run_0/nodes_9_forecasters_90000', '/weak_scaling/run_0/nodes_10_forecasters_100000']\n"
     ]
    }
   ],
   "source": [
    "all_jobs_to_wait = []\n",
    "\n",
    "if GENERATE_DATA:\n",
    "    all_jobs_to_wait.extend(one_milion_test())\n",
    "    all_jobs_to_wait.extend(strong_scaling())\n",
    "    all_jobs_to_wait.extend(weak_scaling())\n",
    "\n",
    "    print(\"Waiting for jobs to finish...\")\n",
    "    print(all_jobs_to_wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_name in tqdm(all_jobs_to_wait):\n",
    "    while not os.path.exists(f\"{data_folder}/{job_name}/timings.txt\"):\n",
    "        time.sleep(10)  # Poll every 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization and Analysis\n",
    "\n",
    "In this section, we load the forecast, weights, and biases data from the one million forecasters test. We then visualize the data using histograms to understand the distribution of these variables. \n",
    "\n",
    "- **Forecast Plot**: We create histograms for each forecast variable across different dimensions.\n",
    "- **Weights Plot**: We generate histograms for the weights across different dimensions.\n",
    "- **Biases Plot**: We plot the histogram for the biases.\n",
    "- **Forecasted Values Plot**: We plot the histograms of the forecasted values and their normalized versions to analyze their distribution and trends.\n",
    "\n",
    "As expected all the weight, biases and forecasted values are normally distributed, since the noise from which they are generated is normally distributed.\n",
    "\n",
    "Moreover, since the model is autorergressive, we can apprecciate the increasing variance of the forecasted values as the time step increases.\n",
    "\n",
    "Finally, we plot the forecasted values and their normalized versions to analyze their distribution and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "one_milion_folder = f\"data/num_forecasters_{one_milion_forecasters}_num_nodes_{one_milion_nodes}_one_milion_test\"\n",
    "\n",
    "forecast = np.load(f\"{one_milion_folder}/forecasting.npy\")\n",
    "weights  = np.load(f\"{one_milion_folder}/weights.npy\")\n",
    "biases   = np.load(f\"{one_milion_folder}/biases.npy\")\n",
    "\n",
    "print(forecast.shape)\n",
    "print(weights.shape)\n",
    "print(biases.shape)\n",
    "\n",
    "n_bins = 25\n",
    "\n",
    "## FORECASTER PLOT\n",
    "variable = forecast\n",
    "fig, axs = plt.subplots(5, 2, figsize=(6, 12))\n",
    "\n",
    "# Flatten the axs array for easy iteration\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot histograms in each subplot\n",
    "for z in range(variable.shape[2]):\n",
    "    for y in range(variable.shape[1]):\n",
    "        f_list = []\n",
    "        for x in range(variable.shape[0]):\n",
    "            f_list.append(variable[x][y][z])\n",
    "        axs[z*variable.shape[1]+y].hist(f_list, bins=n_bins)\n",
    "        axs[z*variable.shape[1]+y].set_title(f'forecast {z*variable.shape[1]+y}')\n",
    "f_list = []\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## WEIGHTS PLOT\n",
    "variable = weights\n",
    "fig, axs = plt.subplots(variable.shape[1], variable.shape[2], figsize=(12, 4))\n",
    "\n",
    "# Flatten the axs array for easy iteration\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot histograms in each subplot\n",
    "for z in range(variable.shape[2]):\n",
    "    for y in range(variable.shape[1]):\n",
    "        f_list = []\n",
    "        for x in range(variable.shape[0]):\n",
    "            f_list.append(variable[x][y][z])\n",
    "        axs[z*variable.shape[1]+y].hist(f_list, bins=n_bins)\n",
    "        axs[z*variable.shape[1]+y].set_title(f'weights {z*variable.shape[1]+y}')\n",
    "f_list = []\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## BIASES PLOT\n",
    "variable = biases\n",
    "\n",
    "# Plot histograms of f_list\n",
    "for x in range(variable.shape[0]):\n",
    "    f_list.append(variable[x][0])\n",
    "\n",
    "plt.hist(f_list, bins=n_bins)\n",
    "#Put a name\n",
    "plt.title('biases')\n",
    "\n",
    "\n",
    "forecasted_values_tot = []\n",
    "## PLOT OF THE FORECASTED VALUES\n",
    "for j in range(forecast.shape[1]):\n",
    "    forecasted = []\n",
    "    for i in range(forecast.shape[0]):\n",
    "        forecasted.append(forecast[i][j][0])\n",
    "    forecasted_values_tot.append(forecasted)\n",
    "\n",
    "# New figure for a new plot\n",
    "plt.figure()\n",
    "# Plot the hist forecasted values in the same graph with different colours\n",
    "for i in range(5):\n",
    "    plt.hist(forecasted_values_tot[i], bins=n_bins, alpha=0.5, label=f'forecast {i}')\n",
    "\n",
    "plt.legend()\n",
    "forecasts_mean = np.mean(forecast, axis=0)\n",
    "\n",
    "# normalize by subtracting the mean\n",
    "forecast_normalized = forecast - forecasts_mean\n",
    "\n",
    "forecasted_values_tot = []\n",
    "## PLOT OF THE FORECASTED VALUES\n",
    "for i in range(1000):\n",
    "    forecasted = []\n",
    "    \n",
    "    for j in range(5):\n",
    "        forecasted.append(forecast_normalized[i][j][0])\n",
    "    \n",
    "    forecasted_values_tot.append(forecasted)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# plot the lines\n",
    "for i in range(1000):\n",
    "    plt.plot(forecasted_values_tot[i], label=f'forecast {i}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong Scalability Test\n",
    "\n",
    "In this section, we analyze the execution times for the strong scalability test. We have already submitted jobs for different numbers of nodes and collected the execution times. The results are plotted on a logarithmic scale to better visualize the differences in execution times as the number of nodes increases.\n",
    "\n",
    "The strong scalability test helps us understand how the execution time decreases as we increase the number of nodes while keeping the problem size constant. Ideally, the execution time should decrease proportionally with the increase in the number of nodes, indicating efficient parallelization and resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_times_strong_scaling = []\n",
    "\n",
    "# Submit jobs for each test configuration\n",
    "for num_nodes in strong_scaling_nodes:\n",
    "    execution_time_file = f\"{data_folder}/num_forecasters_{strong_scaling_forecasters}_num_nodes_{num_nodes}_strong_scaling/timings.txt\"\n",
    "\n",
    "    with open(execution_time_file, \"r\") as f:\n",
    "        execution_time = float(f.read())\n",
    "    execution_times_strong_scaling.append(execution_time)\n",
    "    print(f\"Execution time for {num_nodes} nodes: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.yscale('log')\n",
    "plt.plot(strong_scaling_nodes, execution_times_strong_scaling, label='Strong Scaling', marker='o')\n",
    "plt.xlabel('Number of Nodes')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Strong Scalability Test')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(f\"{charts_folder}/scalability_plot.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Scalability Test\n",
    "\n",
    "In this section, we analyze the execution times for the weak scalability test. \n",
    "\n",
    "The weak scalability test helps us understand how the execution time changes as we increase the number of nodes while keeping the workload per node constant. Ideally, the execution time should remain constant with the increase in the number of nodes, indicating efficient parallelization and resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_times_weak_scaling = []\n",
    "\n",
    "# Submit jobs for each test configuration\n",
    "for num_nodes in weak_scaling_nodes:\n",
    "    execution_time_file = f\"{data_folder}/num_forecasters_{weak_scaling_forecasters*num_nodes}_num_nodes_{num_nodes}_weak_scaling/timings.txt\"\n",
    "\n",
    "    with open(execution_time_file, \"r\") as f:\n",
    "        execution_time = float(f.read())\n",
    "    execution_times_weak_scaling.append(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the weak scalability results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(weak_scaling_nodes, execution_times_weak_scaling, label='Weak Scaling', marker='o')\n",
    "plt.xlabel(\"Number of Nodes\")\n",
    "plt.ylabel(\"Execution Time (seconds)\")\n",
    "plt.title(\"Weak Scalability Test\")\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{charts_folder}/weak_scalability.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we have successfully set up and executed a series of scalability tests for an ensemble of forecasters in a high-performance computing environment. We started by importing the necessary libraries and setting up the directory structure. We then defined functions to submit jobs for one million forecasters, strong scaling, and weak scaling tests.\n",
    "\n",
    "We visualized the data generated from the one million forecasters test, analyzing the distribution of forecast, weights, and biases. The histograms confirmed that these variables are normally distributed, as expected.\n",
    "\n",
    "The strong scalability test demonstrated how the execution time decreases with an increasing number of nodes, indicating efficient parallelization. The weak scalability test showed that the execution time remains relatively constant as the number of nodes increases, suggesting good resource utilization.\n",
    "\n",
    "Overall, these tests provide valuable insights into the performance and scalability of our forecasting ensemble, helping us optimize and improve our high-performance computing workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
